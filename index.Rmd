---
title: "Essential Statistical Skills for Research in Energy and Environmental Technologies"
subtitle: "Workshop for the Faculty of Technology,  University of Sri Jayewardenepura"
author: "Dr Thiyanga S. Talagala"
institute: ""
date: "August 23, 2024"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: 
      - default
      - default-fonts
      - libs/cc-fonts.css
      - libs/figure-captions.css
      - xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#081d58",
  text_bold_color = "#ce1256",
  title_slide_text_color = "#edf8b1",
  header_font_google = google_font("Josefin Sans"),
  base_font_size = "20px",
  text_font_size = "1.5rem",
  code_font_size = "1rem"
 #text_font_google   = google_font("Montserrat", "300", "300i")
 # code_font_google   = google_font("Fira Mono")
)
```

## Essential Statistical Skills

1. Statistical software and programming skills

2. Data wrangling skills

3. Data visualization skills

4. Design and analysis of experiments

4. Statistical modelling skills

5. Machine learning skills

6. Communication skills

---
class: titled, middle

## We have AI and ChatGPT.

## We don't need these skills anymore.

---
class: titled, middle

## ~~We have AI and ChatGPT~~.

## ~~We don't need these skills anymore~~.

# AI-based tools are powerful, but it is most effective **when used in combination with human expertise.**

---
class: titled, middle

## ~~We have AI and ChatGPT~~.

## ~~We don't need these skills anymore~~.

##  AI-based tools are powerful, but it is most effective **when used in combination with human expertise.**

- **Improve prompt engineering:** Knowing the underlying concepts helps in framing better questions and guiding AI to give accurate and relevant outputs

- **Evaluate AI-generated outputs:**  To critically assess whether AI-generated results are valid and make sense for your problem

---
class: inverse, center, middle

# Example 1

---
.pull-left[
Determine **which natural seed (Species A, B, or C) is most effective in reducing turbidity** in water samples.

- The tests were carried out, using artificial turbid
water with conventional jar test apparatus.

> Response variable = Turbidity Reduction Efficiency (TRE %)

]

.pull-right[

![](experiment.png)
]


---

.pull-left[

## Data

```{r, echo=FALSE}
Treatment <- c("A", "B", "C")
TRE <- c(10, 55, 80)
df <- data.frame(Tratment=Treatment,
                 TRE = TRE)
df |> knitr::kable()
```

- Statistical conclusions cannot be drawn based on one replicate.

]

.pull-right[

![](experiment.png)

]


---

.pull-left[

- Independent Variable (Treatment): Type of natural seed added to the water.

    - Treatment A: Seed A
    - Treatment B: Seed B
    - Treatment C: Seed C

- Dependent Variable: Turbidity Reduction Efficiency

- Randomization

- Replication


]


.pull-right[


![](image2.png)
]

---

## Completely Randomized Design (CRD)

.pull-left[


![](image2.png)

]

---

##  Common Experimental Designs

.pull-left[

- Completely Randomized Design (CRD)

- Randomized Complete Block Design (RCBD)

- Latin Square Design

- Factorial Design

- Split-Plot Design

- Nested Design

]

---

class: inverse, center, middle

# Example 2

---

## Chicago River: Water Quality Monitoring

.pull-left[

![](image4.jpg)

]

.pull-right[

![](image5.jpg)
]

- Measure the pH level of water at a specific location daily at 10 PM over a two-year period.

---
.pull-left[
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load necessary libraries
library(dplyr)
library(kableExtra)

# Set the start and end dates
start_date <- as.Date("2022-01-01")
end_date <- as.Date("2023-12-31")

# Generate a sequence of dates for two years
date_column <- seq.Date(from = start_date, to = end_date, by = "day")

# Generate random pH values between 6.5 and 8.5 (common range for natural water)
set.seed(123) # For reproducibility
pH_values <- round(runif(length(date_column), min = 6.5, max = 8.5), 2)

# Create the dataset
data <- data.frame(
  Date = date_column,
  pH = pH_values
)
kable(data, caption = "pH Data ", align = "c") |>
  kable_styling(bootstrap_options = "striped", full_width = FALSE, font_size = 14) |>
  column_spec(1, width = "4cm") |>
  column_spec(2, width = "3cm")
```
]

.pull-right[

## Time series data

- Sequence of data points recorded at regular time intervals. 

- Temporal dependency: Data points are dependent on time
]


---
class: inverse, center, middle

# Example 3

---

## Spatial data

.pull-left[

![](image4.jpg)

]



.pull-right[

![](image5.jpg)

]

- Measure the pH level of water at multiple locations on a given day.


--- 
class: inverse, center, middle

# Example 4

---

## Spatio-temporal data

.pull-left[

![](image4.jpg)

]



.pull-right[

![](image5.jpg)

]

- Measure the pH level of water at multiple locations daily at 10 PM over a two-year period.

---


![](ip11.png)

---
class: inverse, middle, center

## Nature of your research problem


---

## Statistical Inference: (Point and Interval Estimates)

Suppose you collect daily pH measurements from the Chicago River over a year. You want to **estimate** the average pH level of the river for the year.

> Point Estimate

> Interval Estimate (Confidence Interval)

Eg:  A 95% confidence interval for the mean pH might be (7.0, 7.4). 

---

## Statistical Inference:  Hypothesis Testing

Suppose you are analyzing water quality data from the Chicago River and want to determine whether there is a significant difference in the average pH levels during the winter and summer months.

$$H_0: \mu_{winter} = \mu_{summer}$$

$$H_1: \mu_{winter} \neq \mu_{summer}$$

---

## Hypothesis testing: Parametric vs Non-parametric

.pull-left[

### Parametric

- One-Sample t-Tests

- Independent Two-Sample t-Test

- Paired t-Test

- One-Way ANOVA

- Two-way ANOVA



]


.pull-right[

### Non-parametric

- Chi-Square Test of Independence

- Chi-Square Goodness-of-Fit Test

- Mann-Whitney U Test

- Wilcoxon Signed-Rank Test 

- Kruskal-Wallis H Test

- Friedman Test

]

---

## Forecasting

Measure the pH level of water at a specific location daily at 10 PM over a two-year period. You are interested in forecasting the  pH level in the upcoming months based on historical data.

---

## Spatial Interpolation

Suppose you have pH level measurements from various monitoring stations along the Chicago River, and you want to estimate the pH levels at unobserved locations between these stations.

---

## Cluster Analysis Problem

- You have collected water quality data from various locations along the Chicago River, including multiple parameters such as pH, dissolved oxygen (DO), turbidity, temperature, and nitrate concentration. 

- The goal is to identify clusters of locations with similar water quality profiles, which could help in identifying areas with potential pollution sources or regions requiring special attention for water management.

---

## Making Predictions

Predict the pH level of a river location based on various environmental and infrastructural features.



---

.pull-left[

### Supervised Learning

**Dependent Variable:** pH level

**Independent variables:**

-  The count of industrial establishments near the location

- Average rainfall

- Average temperature

- Land use

- Soil type ( (e.g., agriculture, urban, forest))

]


.pull-right[

### Unsupervised Learning


**Independent variables:**

- pH level

-  The count of industrial establishments near the location

- Average rainfall

- Average temperature

- Land use

- Soil type ( (e.g., agriculture, urban, forest))


]

---


.pull-left[

### Supervised Learning

- Linear Regression 

- Logistic Regression

- Decision Trees

- Random Forest 

- Support Vector Machines (SVM)

- k-Nearest Neighbors (k-NN)

- Naive Bayes

- XGBoost

]

.pull-right[

### Unsupervised Learning

- Clustering

- Principal Component Analysis (PCA)

- Factor Analysis

- Association Rules

]

---

## Descriptive Statistics

- Measures of Location: describe the central tendency or typical value of a dataset (Mean, Median, Mode)

- Measures of Dispersion:  describe the spread or variability of the dataset (Variance, Standard Deviation, Interquartile Range)

- Measures of Shape: describe the asymmetry and degree of peakness of the distribution (Skewness, Kurtosis)

- Measures of Association: describe the strength and direction of relationships between variables (Spearman's correlation, Pearson's correlation coefficient)

---

.pull-left[
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Generate sample data 
set.seed(25052020)
# normal
s1 <- rnorm(100)
s2 <- rbeta(100, 2, 1)
s3 <- rexp(100)
# mixture
s4 <- rnorm(100, rep(c(-1, 1), each = 50) * sqrt(0.9), sqrt(0.1))
four <- data.frame(
dist = factor(rep(c("s1", "s2", "s3", "s4"),
each = 100),
c("s1", "s2", "s3", "s4")),
vals = c(s1, s2, s3, s4)
)
df <- data.frame(s1=s1, s2=s2, s3=s3, s4=s4)
library(ggplot2)
ggplot(four, aes(x = vals)) + 
geom_histogram(aes(fill = dist), colour = "black") + 
coord_cartesian(xlim = c(-3, 6)) + 
facet_wrap(~ dist) 

```

]

.pull-right[
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load necessary libraries
library(dplyr)
library(knitr)
library(moments)  # For skewness function

# Create a function to calculate descriptive statistics
calc_stats <- function(data) {
  data %>%
    summarise(
      Mean = round(mean(vals), 2),
      Median = round(median(vals), 2),
      Variance = round(var(vals), 2),
      SD = round(sd(vals), 2),
      Skewness = round(skewness(vals), 2)
    )
}
# Calculate statistics for each distribution
stats <- four %>%
  group_by(dist) %>%
  do(calc_stats(.)) %>%
  ungroup()

# Display results in a kable
kable(stats, caption = "Descriptive Statistics for Each Distribution") |>
  kable_styling(font_size = 20) 
```
]

---
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load necessary libraries
library(ggplot2)
library(gridExtra)

set.seed(42)  # For reproducibility

# Function to generate data with a specific Pearson correlation
generate_data <- function(r, n = 100) {
  x <- rnorm(n)
  y <- r * x + sqrt(1 - r^2) * rnorm(n)
  data.frame(x = x, y = y)
}

# Generate data for each Pearson correlation
data_1 <- generate_data(1)
data_08 <- generate_data(0.8)
data_05 <- generate_data(0.5)
data_neg05 <- generate_data(-0.5)
data_neg08 <- generate_data(-0.8)
data_neg1 <- generate_data(-1)

# Create scatter plots
plot_1 <- ggplot(data_1, aes(x = x, y = y)) +
  geom_point(color = 'blue') +
  ggtitle("Pearson r = +1") +
  theme_minimal()

plot_08 <- ggplot(data_08, aes(x = x, y = y)) +
  geom_point(color = 'green') +
  ggtitle("Pearson r = +0.8") +
  theme_minimal()

plot_05 <- ggplot(data_05, aes(x = x, y = y)) +
  geom_point(color = 'purple') +
  ggtitle("Pearson r = +0.5") +
  theme_minimal()

plot_neg05 <- ggplot(data_neg05, aes(x = x, y = y)) +
  geom_point(color = 'orange') +
  ggtitle("Pearson r = -0.5") +
  theme_minimal()

plot_neg08 <- ggplot(data_neg08, aes(x = x, y = y)) +
  geom_point(color = 'red') +
  ggtitle("Pearson r = -0.8") +
  theme_minimal()

plot_neg1 <- ggplot(data_neg1, aes(x = x, y = y)) +
  geom_point(color = 'darkred') +
  ggtitle("Pearson r = -1") +
  theme_minimal()

# Arrange the plots in a grid
grid.arrange(plot_1, plot_08, plot_05, plot_neg05, plot_neg08, plot_neg1, ncol = 3)

```


---

.pull-left[
```{r, echo=FALSE, warning=FALSE}
library(datasauRus)
library(ggplot2)
  ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset))+
    geom_point()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 3)
```
]

.pull-right[

**Among the scatter plots, which one exhibits the highest correlation coefficient?**

]
---


## Data Visualisation

Data visualization is a fundamental prerequisite for any data analysis project. 


- Visualization helps you gain an immediate understanding of the dataset

- Visualizing the data helps in identifying missing data, errors, or inconsistencies that might need to be addressed before moving forward with analysis.

- Effective visualizations allow analysts to communicate complex results clearly to stakeholders.

---

![](![](https://blog.revolutionanalytics.com/downloads/DataSaurus%20Dozen.gif)


---

## Data Analysis with R: Lab Session

- Introduction to R and RStudio

- Data Visualization with R

- Introduction to hypothesis testing 

Resources: https://vizmaster.netlify.app/note/

---

A chemist wants to measure the bias in a pH meter. She uses the meter to measure the pH in 14 neutral
substances (pH=7) and obtains the data below.

 7.01, 7.04, 6.97, 7.00, 6.99, 6.97, 7.04, 7.04, 7.01, 7.00, 6.99, 7.04, 7.07, 6.97

Is there sufficient evidence to support the claim that the pH meter is not correctly calibrated at the $\alpha$ = 0.05 level of significance?

---

```{r}
ph <- c( 7.01, 7.04, 6.97, 7.00, 6.99, 6.97, 7.04, 7.04, 7.01, 7.00, 6.99, 7.04, 7.07, 6.97)
```


---

```{r, fig.height=5, fig.width=5}
ph.df <- data.frame(pH=ph)
ggplot(ph.df,
aes(sample=pH))+
stat_qq() + stat_qq_line()+labs(x="Theoretical Quantiles", y="Sample Quantiles")
shapiro.test(ph.df$pH)

```

---

H0: Data are normally distributed.

H1: Data are not normally distributed

---

$H_0: \mu = 7$

$H_1: \mu \neq 7$


```{r}
t.test(ph.df$pH, alternative = "two.sided", mu=7)
```
